# üí≥ Credit Card Fraud Detection using Machine Learning

## üìå Problem Statement
Detect fraudulent credit card transactions using machine learning techniques.

## üìä Dataset
- 284,807 transactions
- 492 fraud cases
- Highly imbalanced dataset

## ‚öô Technologies Used
- Python
- Pandas
- Scikit-learn
- SMOTE
- Logistic Regression
- Random Forest

## üß† Models Implemented
1. Logistic Regression
2. Random Forest

## üìà Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-Score
- ROC-AUC Score

## üöÄ Key Highlights
- Handled class imbalance using SMOTE
- Compared multiple ML models
- Evaluated performance using ROC-AUC
- Built complete end-to-end ML pipeline
# Credit-Card-Fraud-Detection-ML
Machine Learning project to detect fraudulent credit card transactions using Logistic Regression and Random Forest.
# ==============================
# 1Ô∏è‚É£ Import Required Libraries
# ==============================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score

from imblearn.over_sampling import SMOTE


# ==============================
# 2Ô∏è‚É£ Load Dataset
# ==============================

df = pd.read_csv("data/creditcard.csv")

print("Dataset Loaded Successfully ‚úÖ")
print("Shape:", df.shape)

print("\nDataset Info:")
print(df.info())

print("\nMissing Values:")
print(df.isnull().sum())

print("\nClass Distribution:")
print(df['Class'].value_counts())


# ==============================
# 3Ô∏è‚É£ Data Visualization
# ==============================

plt.figure(figsize=(6, 4))
sns.histplot(df['Amount'], bins=50)
plt.title("Transaction Amount Distribution")
plt.xlabel("Amount")
plt.ylabel("Count")
plt.show()


# ==============================
# 4Ô∏è‚É£ Feature Scaling
# ==============================

scaler = StandardScaler()
df['Amount_Scaled'] = scaler.fit_transform(df[['Amount']])

# Drop original Amount column
df = df.drop('Amount', axis=1)


# ==============================
# 5Ô∏è‚É£ Define Features & Target
# ==============================

X = df.drop('Class', axis=1)
y = df['Class']

print("\nFeature shape:", X.shape)
print("Target shape:", y.shape)


# ==============================
# 6Ô∏è‚É£ Train-Test Split
# ==============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("\nTraining set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

print("\nTraining Class Distribution:")
print(y_train.value_counts())

print("\nTesting Class Distribution:")
print(y_test.value_counts())


# ==============================
# 7Ô∏è‚É£ Apply SMOTE (Only on Training Data)
# ==============================

smote = SMOTE(random_state=42)

X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

print("\nBefore SMOTE:")
print(y_train.value_counts())

print("\nAfter SMOTE:")
print(y_train_sm.value_counts())


# ==============================
# 8Ô∏è‚É£ Train Logistic Regression
# ==============================

print("\n==============================")
print("Training Logistic Regression...")
print("==============================")

log_model = LogisticRegression(max_iter=1000, solver='liblinear')
log_model.fit(X_train_sm, y_train_sm)

y_pred_log = log_model.predict(X_test)
y_prob_log = log_model.predict_proba(X_test)[:, 1]

print("\nLogistic Regression Results")
print("------------------------")
print("Accuracy:", accuracy_score(y_test, y_pred_log))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_log))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_log))

print("ROC-AUC Score:", roc_auc_score(y_test, y_prob_log))


# ==============================
# 9Ô∏è‚É£ Train Random Forest
# ==============================

print("\n==============================")
print("Training Random Forest...")
print("==============================")

rf_model = RandomForestClassifier(n_estimators=50, random_state=42)
rf_model.fit(X_train_sm, y_train_sm)

y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]

print("\nRandom Forest Results")
print("------------------------")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

print("ROC-AUC Score:", roc_auc_score(y_test, y_prob_rf))
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(log_model, X_test, y_test)
plt.title("Logistic Regression Confusion Matrix")
plt.show()
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test)
plt.title("Random Forest Confusion Matrix")
plt.show()
from sklearn.metrics import roc_curve

fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)

plt.figure(figsize=(6,6))
plt.plot(fpr_log, tpr_log, label="Logistic Regression")
plt.plot(fpr_rf, tpr_rf, label="Random Forest")
plt.plot([0,1],[0,1],'k--')

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()
